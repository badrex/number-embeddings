{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/badr/embeddings/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import faiss \n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#from sentence_transformers.quantization import quantize_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "number_lexicalizer = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'five'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_lexicalizer.number_to_words(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_evaluate = {\n",
    "    \"LaBSE\": \"sentence-transformers/LaBSE\",\n",
    "    \"miniLM-L12\": \"sentence-transformers/all-MiniLM-L12-v2\",\n",
    "    \"miniLM-L6\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"mxbai\": \"mixedbread-ai/mxbai-embed-large-v1\",\n",
    "    \"jina-base\": \"jinaai/jina-embeddings-v2-base-en\",\n",
    "    \"jina-small\": \"jinaai/jina-embeddings-v2-small-en\",\n",
    "    \"jina-code\": \"jinaai/jina-embeddings-v2-base-code\",\n",
    "    \"textCLIP\": \"sentence-transformers/clip-ViT-B-32\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4037453c17455f82026a2f8c3e00bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653d436cae884de39d9035b3142d9a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181e9fccdd1b49c98da2408be3eafeed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eeee59b1f394a5c93546b6cfd5a5714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207b1f88646349628fa648cc97d782a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/804 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d967f7ee5b248fe8c37d0084c14a3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa26e8593674ef1a00cfa6b9efadb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/397 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346785cdae624afbb9ba54ab2465a0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/5.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff89d090aee409f93726018181d0c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6029e8c00b0c4febb492d955a1c9f673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fee8ac85eab47c89e97d6382ec25ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c2f178cf2a4294b8a4719be32da92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/config.json:   0%|          | 0.00/114 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd3ce1e248a4ac1bda2a5c8d31a5d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/badr/embeddings/lib/python3.11/site-packages/sentence_transformers/models/Dense.py:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(input_path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d55cd6f3a6450ba7e43ca2f62b0aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41560f2f4a7443aebb16eba4dc811898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784f3b95a00a4afdad876e15e23d7815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.91k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3bba9ca512b4c3caab3d9d3374f2c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0_CLIPModel/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f850d67fce294516b428ca7a20df6183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0_CLIPModel/vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd5073328854b7385092f7855d95a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0_CLIPModel/preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d911a11f2d0c464da2272fae9bf62fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651be062ba3d4c7398fe158ee48a4e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0_CLIPModel/tokenizer_config.json:   0%|          | 0.00/604 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9babc0892cd74529bb48774a9dfcb638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0_CLIPModel/special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df27c80e2f27417cac42c3fe3164bb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0_CLIPModel/config.json:   0%|          | 0.00/4.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder_models = defaultdict()\n",
    "\n",
    "for m in models_to_evaluate:\n",
    "    encoder_models[m] = SentenceTransformer(\n",
    "        models_to_evaluate[m], \n",
    "        trust_remote_code=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the file resturants.list \n",
    "# and convert to list\n",
    "data = pd.read_csv(\"restaurants.list\", sep=\"\\t\", header=None).to_dict()[0]\n",
    "RESTAURANTS = list(data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {\n",
    "    \"restaurants\": {\n",
    "        \"query\": [\n",
    "            \"Find restaurants that are rated with at least {} {}\", \n",
    "            \"List all restaurants with {} {} rating or higher\",\n",
    "            \"I am looking for restaurants with at least {} {} rating\",\n",
    "            \"Show me restaurants that have {} {} rating or higher\",\n",
    "            \"Which restaurants have at least {} {} rating\",\n",
    "            \"Restaurants with at least {} {} rating\",\n",
    "            \"List restaurants with at least {} {} rating\",\n",
    "            \"Restaurants that have {} {} rating or higher\",\n",
    "            \"I want to see great restaurants with at least {} {} rating\",\n",
    "            \"Give me suggestions for restaurants with {} {} rating or higher\",\n",
    "            \"I want to know which restaurants have at least {} {} rating\",\n",
    "            \"Which restaurants have {} {} rating or higher\",\n",
    "            \"Great restaurants that have at least {} {} rating\",\n",
    "            \"Show me of restaurants with at least {} {} rating\",\n",
    "        ], \n",
    "        \"candidate\": \"{} restaurant has {} stars rating.\",\n",
    "        \"attribute\": \"stars\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_case(\n",
    "        attribute: str,\n",
    "        search_items: List[str],\n",
    "        query_template: str, \n",
    "        candidate_template: str, \n",
    "        max_items_to_retrieve: int=11) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create a test case for the evaluation.\n",
    "    :param attribute: the attribute to be queried (e.g., \"stars\", \"awards\")\n",
    "    :param search_items: a list of items to search over (e.g., restaurants)\n",
    "    :param query_template: a query template to be used\n",
    "    :param candidate_template: a candidate template to be used\n",
    "    :return: a tuple of query sentence, candidate sentences, and hit flags\n",
    "    \"\"\"\n",
    "    # test if input parameters are valid\n",
    "    assert attribute in [\"stars\", \"awards\"], \"Invalid attribute\"\n",
    "    assert len(search_items) >= 1, \"Provide at least one search items\"\n",
    "\n",
    "    # sample a target number \n",
    "    target_number = np.random.randint(6, 10)\n",
    "\n",
    "    # associate a value for each search item\n",
    "    # where only N of those are equal to or higher than the target number\n",
    "    items_to_retrieve = np.random.randint(1, max_items_to_retrieve)\n",
    "\n",
    "    hit_ratings = np.random.randint(target_number, 10, items_to_retrieve)\n",
    "    miss_ratings = np.random.randint(\n",
    "        1, target_number, len(search_items) - items_to_retrieve\n",
    "    )\n",
    "    \n",
    "    item_ratings = np.concatenate([hit_ratings, miss_ratings])\n",
    "\n",
    "    # define a boolean list to check if the rating is hit (should be returned)\n",
    "    relevance_score = [\n",
    "        0 if rating < target_number else 1 for rating in item_ratings\n",
    "    ]\n",
    "\n",
    "    # create the query sentence\n",
    "    query_sentence = query_template.format(target_number, attribute)\n",
    "    target_number_lex = number_lexicalizer.number_to_words(target_number)\n",
    "    query_sentence_lex = query_template.format(target_number_lex, attribute)\n",
    "\n",
    "    candidates = [\n",
    "        candidate_template.format(restaurant, rating)\n",
    "        for restaurant, rating in zip(search_items, item_ratings)\n",
    "    ]\n",
    "\n",
    "    candidates_lex = [\n",
    "        candidate_template.format(\n",
    "            restaurant, \n",
    "            number_lexicalizer.number_to_words(rating))\n",
    "        for restaurant, rating in zip(search_items, item_ratings)\n",
    "    ]\n",
    "\n",
    "\n",
    "    return { \n",
    "        \"query\": {\n",
    "            \"numeral\": query_sentence,\n",
    "            \"lexical\": query_sentence_lex\n",
    "        }, \n",
    "        \"candidates\": {\n",
    "            \"numeral\": candidates,\n",
    "            \"lexical\": candidates_lex\n",
    "        },\n",
    "        \"relevance_score\": relevance_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = create_test_case(\n",
    "    \"stars\", \n",
    "    [\"Taj Mahal\", \"Burger King\", \"McDonald's\", \"KFC\", \"Pizza Hut\", \"Subway\", \n",
    "     \"Greggs\", \"Pret A Manger\", \"Nando's\", \"Starbucks\", \"Costa\"],\n",
    "    templates[\"restaurants\"][\"query\"][0],\n",
    "    templates[\"restaurants\"][\"candidate\"], \n",
    "    max_items_to_retrieve=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to evaluate a single test case and return precision and recall at 10\n",
    "def evaluate_test_case(\n",
    "        query: Dict[str, str], \n",
    "        candidates: List[str], \n",
    "        relevance_score: List[int], \n",
    "        model: SentenceTransformer) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a test case using a given model.\n",
    "    :param query: a query sentence\n",
    "    :param candidates: a list of candidate sentences\n",
    "    :param relevance_score: a list of relevance scores\n",
    "    :param model: a sentence transformer model to use\n",
    "    :return: a tuple of precision and recall at 10\n",
    "    \"\"\"\n",
    "    # encode the query and candidates\n",
    "    query_embedding = model.encode(query)\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "    # compute the cosine similarity between the query and candidates\n",
    "    similarity = np.dot(candidate_embeddings, query_embedding.T)\n",
    "\n",
    "    # rank the candidates based on the similarity\n",
    "    ranked_indices = np.argsort(similarity, axis=0)[::-1]\n",
    "\n",
    "    # retrieve the relevance scores based on the ranking\n",
    "    ranked_relevance = np.array(relevance_score)[ranked_indices]\n",
    "\n",
    "    # compute precision and recall at 10\n",
    "    precision_at_10 = np.mean(ranked_relevance[:10])\n",
    "    recall_at_10 = np.sum(ranked_relevance[:10]) / np.sum(relevance_score)\n",
    "\n",
    "    return precision_at_10, recall_at_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3, 1.0)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_test_case(t[\"query\"][\"numeral\"], t[\"candidates\"][\"numeral\"], t[\"relevance_score\"], encoder_models[\"LaBSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_to_emoji = {0: \"✖\", 1: \"✅\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create test case 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dd269b0f00436ca78e1bbc6e3506e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LaBSE\n",
      "Average P@10: 0.207\n",
      "Average R@10: 0.302\n",
      "--------------------------------------------------------------------------------\n",
      "Create test case 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29bb26cfa624a7a9bad28e5dfd9e5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LaBSE\n",
      "Average P@10: 0.207\n",
      "Average R@10: 0.328\n",
      "--------------------------------------------------------------------------------\n",
      "Create test case 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43558d22af9448debcd693ac6b733ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LaBSE\n",
      "Average P@10: 0.221\n",
      "Average R@10: 0.387\n",
      "--------------------------------------------------------------------------------\n",
      "Create test case 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d5fa09a5e549f9976c1ef75b423db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LaBSE\n",
      "Average P@10: 0.229\n",
      "Average R@10: 0.504\n",
      "--------------------------------------------------------------------------------\n",
      "Create test case 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d24fbc51ff423f8f9dc972acfe48c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LaBSE\n",
      "Average P@10: 0.221\n",
      "Average R@10: 0.491\n",
      "--------------------------------------------------------------------------------\n",
      "Create test case 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f797803b7e49efb83f3ac568e1328b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LaBSE\n",
      "Average P@10: 0.243\n",
      "Average R@10: 0.401\n",
      "--------------------------------------------------------------------------------\n",
      "Create test case 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0cd76d0f6494b48b8514798cfe71086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LaBSE\n",
      "Average P@10: 0.207\n",
      "Average R@10: 0.394\n",
      "--------------------------------------------------------------------------------\n",
      "Create test case 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fef6e8adb14799a714e055524a06ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LaBSE\n",
      "Average P@10: 0.157\n",
      "Average R@10: 0.265\n",
      "--------------------------------------------------------------------------------\n",
      "Create test case 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad685d0bf6e2440dadd5ee812be8d075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LaBSE\n",
      "Average P@10: 0.186\n",
      "Average R@10: 0.396\n",
      "--------------------------------------------------------------------------------\n",
      "Create test case 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e445b0f51f4ddba5909c51e11f0062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LaBSE\n",
      "Average P@10: 0.186\n",
      "Average R@10: 0.393\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# iterate over the templates and create test cases\n",
    "search_items = {\"restaurant\": RESTAURANTS}\n",
    "\n",
    "\n",
    "model_name = \"LaBSE\"\n",
    "model = encoder_models[model_name]\n",
    "\n",
    "for k in range(10):\n",
    "    #print(f\"Experiment {k+1}\")\n",
    "    test_cases = defaultdict(list)\n",
    "\n",
    "    print(f'Create test case {k + 1}')\n",
    "\n",
    "    for search_need in templates:\n",
    "        for query_template in templates[search_need][\"query\"]:\n",
    "            query, candidates, relevance_scores = create_test_case(\n",
    "                templates[search_need][\"attribute\"], \n",
    "                RESTAURANTS, \n",
    "                query_template, \n",
    "                templates[search_need][\"candidate\"], \n",
    "                lexicalized=True\n",
    "            )\n",
    "\n",
    "            test_cases[search_need].append(\n",
    "                {\n",
    "                    \"query\": query,\n",
    "                    \"candidates\": candidates,\n",
    "                    \"relevance_scores\": relevance_scores\n",
    "                }\n",
    "            )\n",
    "\n",
    "    precision_at_10_values, recall_at_10_values = [], []\n",
    "\n",
    "\n",
    "    # iterate over the test cases\n",
    "    for t_case in tqdm(test_cases[\"restaurants\"]):\n",
    "        # get the query and the candidates\n",
    "        query, candidates = t_case[\"query\"], t_case[\"candidates\"]\n",
    "        relevance_scores = t_case[\"relevance_scores\"]\n",
    "\n",
    "        # encode the query and the candidates\n",
    "        query_embedding = model.encode(query).reshape(1, -1)\n",
    "        candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "        # L2 normalize the embeddings\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        faiss.normalize_L2(candidate_embeddings)\n",
    "\n",
    "        # get dimensions of the embeddings\n",
    "        d = query_embedding.shape[1]\n",
    "\n",
    "        # make search index\n",
    "        index = faiss.IndexFlatIP(d)\n",
    "        index.add(candidate_embeddings)\n",
    "\n",
    "        # search the index\n",
    "        k = 10\n",
    "\n",
    "        D, I = index.search(query_embedding, k)\n",
    "\n",
    "        if debug:\n",
    "\n",
    "            print(f\"Query: {query}\")\n",
    "            print()\n",
    "\n",
    "            for i in range(k):\n",
    "                retrieved_sentence = candidates[I[0][i]]\n",
    "                hit_or_not = relevance_scores[I[0][i]]\n",
    "                print(f\"Rank {i+1:>2}: {retrieved_sentence:60} \", end=\"\")\n",
    "                print(f\"{D[0][i]:>5.4f} \", end=\"\")\n",
    "                print(f\"{relevance_to_emoji[hit_or_not]:>3}\")\n",
    "\n",
    "\n",
    "        # compute precision and recall at 10\n",
    "        retrieved_items = sum(np.array(relevance_scores)[I[0]])\n",
    "\n",
    "        precision_at_10 = retrieved_items / k\n",
    "        recall_at_10 = retrieved_items / np.sum(relevance_scores)\n",
    "\n",
    "        precision_at_10_values.append(precision_at_10)\n",
    "        recall_at_10_values.append(recall_at_10)\n",
    "\n",
    "        if debug:\n",
    "\n",
    "            print()\n",
    "            print(f\"Total num of relevant items: {np.sum(relevance_scores)}\")\n",
    "            print(f\"Total num of retrieved items: {retrieved_items}\")\n",
    "\n",
    "            print()\n",
    "            print(f\"P@10: {precision_at_10:.3f}\")\n",
    "            print(f\"R@10: {recall_at_10:.3f}\")\n",
    "            print()\n",
    "\n",
    "    # caluclate the average precision and recall at 10\n",
    "    avg_precision_at_10 = np.mean(precision_at_10_values)\n",
    "    avg_recall_at_10 = np.mean(recall_at_10_values)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Average P@10: {avg_precision_at_10:.3f}\")\n",
    "    print(f\"Average R@10: {avg_recall_at_10:.3f}\")\n",
    "\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_cases[\"restaurants\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LaBSE\n",
      "Average P@10: 0.130\n",
      "Average R@10: 0.220\n",
      "--------------------------------------------------------------------------------\n",
      "Model: miniLM-L12\n",
      "Average P@10: 0.180\n",
      "Average R@10: 0.428\n",
      "--------------------------------------------------------------------------------\n",
      "Model: miniLM-L6\n",
      "Average P@10: 0.240\n",
      "Average R@10: 0.525\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# encode the query and the candidates\u001b[39;00m\n\u001b[1;32m     15\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(query)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m candidate_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# L2 normalize the embeddings\u001b[39;00m\n\u001b[1;32m     19\u001b[0m faiss\u001b[38;5;241m.\u001b[39mnormalize_L2(query_embedding)\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:517\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    514\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 517\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    519\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:118\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    116\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    121\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    685\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m         output_attentions,\n\u001b[1;32m    692\u001b[0m     )\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:626\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    623\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    624\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 626\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/transformers/pytorch_utils.py:238\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:638\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 638\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    538\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/embeddings/lib/python3.11/site-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# evalauting the models\n",
    "debug = False\n",
    "\n",
    "for model_name, model in encoder_models.items():\n",
    "\n",
    "    precision_at_10_values, recall_at_10_values = [], []\n",
    "\n",
    "    # iterate over the test cases\n",
    "    for t_case in test_cases[\"restaurants\"]:\n",
    "        # get the query and the candidates\n",
    "        query, candidates = t_case[\"query\"], t_case[\"candidates\"]\n",
    "        relevance_scores = t_case[\"relevance_scores\"]\n",
    "\n",
    "        # encode the query and the candidates\n",
    "        query_embedding = model.encode(query).reshape(1, -1)\n",
    "        candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "        # L2 normalize the embeddings\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        faiss.normalize_L2(candidate_embeddings)\n",
    "\n",
    "        # get dimensions of the embeddings\n",
    "        d = query_embedding.shape[1]\n",
    "\n",
    "        # make search index\n",
    "        index = faiss.IndexFlatIP(d)\n",
    "        index.add(candidate_embeddings)\n",
    "\n",
    "        # search the index\n",
    "        k = 10\n",
    "\n",
    "        D, I = index.search(query_embedding, k)\n",
    "\n",
    "        if debug:\n",
    "\n",
    "            print(f\"Query: {query}\")\n",
    "            print()\n",
    "\n",
    "            for i in range(k):\n",
    "                retrieved_sentence = candidates[I[0][i]]\n",
    "                hit_or_not = relevance_scores[I[0][i]]\n",
    "                print(f\"Rank {i+1:>2}: {retrieved_sentence:60} \", end=\"\")\n",
    "                print(f\"{D[0][i]:>5.4f} \", end=\"\")\n",
    "                print(f\"{relevance_to_emoji[hit_or_not]:>3}\")\n",
    "\n",
    "\n",
    "        # compute precision and recall at 10\n",
    "        retrieved_items = sum(np.array(relevance_scores)[I[0]])\n",
    "\n",
    "        precision_at_10 = retrieved_items / k\n",
    "        recall_at_10 = retrieved_items / np.sum(relevance_scores)\n",
    "\n",
    "        precision_at_10_values.append(precision_at_10)\n",
    "        recall_at_10_values.append(recall_at_10)\n",
    "\n",
    "        if debug:\n",
    "\n",
    "            print()\n",
    "            print(f\"Total num of relevant items: {np.sum(relevance_scores)}\")\n",
    "            print(f\"Total num of retrieved items: {retrieved_items}\")\n",
    "\n",
    "            print()\n",
    "            print(f\"P@10: {precision_at_10:.3f}\")\n",
    "            print(f\"R@10: {recall_at_10:.3f}\")\n",
    "            print()\n",
    "\n",
    "    # caluclate the average precision and recall at 10\n",
    "    avg_precision_at_10 = np.mean(precision_at_10_values)\n",
    "    avg_recall_at_10 = np.mean(recall_at_10_values)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Average P@10: {avg_precision_at_10:.3f}\")\n",
    "    print(f\"Average R@10: {avg_recall_at_10:.3f}\")\n",
    "\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Replacement index 1 out of range for positional args tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m target_number \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      3\u001b[0m query_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShow me restaurants with at least \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m rating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m query_sentence \u001b[38;5;241m=\u001b[39m \u001b[43mquery_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# create a list of randome ratings between 1 and 10\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# where only 10 of those are equal to or higher than the target number\u001b[39;00m\n\u001b[1;32m      8\u001b[0m hit_ratings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(target_number, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Replacement index 1 out of range for positional args tuple"
     ]
    }
   ],
   "source": [
    "# sample target number\n",
    "target_number = np.random.randint(6, 10)\n",
    "query_template = \"Show me restaurants with at least {} {} rating\"\n",
    "query_sentence = query_template.format(target_number)\n",
    "\n",
    "# create a list of randome ratings between 1 and 10\n",
    "# where only 10 of those are equal to or higher than the target number\n",
    "hit_ratings = np.random.randint(target_number, 11, 10)\n",
    "miss_ratings = np.random.randint(1, target_number, len(RESTAURANTS) - 10)\n",
    "all_ratings = np.concatenate([hit_ratings, miss_ratings])\n",
    "\n",
    "#ratings = np.random.randint(1, 11, len(RESTAURANTS))\n",
    "\n",
    "\n",
    "candidate_template = \"{} restaurant has {} stars rating.\"\n",
    "\n",
    "# define a boolean list to check if the rating is hit (should be returned)\n",
    "is_hit = [\n",
    "    0 if rating < target_number else 1 for rating in all_ratings\n",
    "]\n",
    "\n",
    "candidate_sentences = [\n",
    "        candidate_template.format(restaurant, rating)\n",
    "        for restaurant, rating in zip(RESTAURANTS, all_ratings)\n",
    "]\n",
    "\n",
    "hit_to_emoji = {0: \"✖\", 1: \"✅\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(is_hit) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holy Cannoli! restaurant has 8 stars rating.                      ✅\n",
      "Sushi Samurai restaurant has 7 stars rating.                      ✅\n",
      "Nacho Average Taco restaurant has 8 stars rating.                 ✅\n",
      "Curry Up Now restaurant has 8 stars rating.                       ✅\n",
      "Oui, Chef! restaurant has 10 stars rating.                        ✅\n",
      "The Souvlaki Shack restaurant has 10 stars rating.                ✅\n",
      "Kimchi Commandos restaurant has 9 stars rating.                   ✅\n",
      "Pad Thai Guy restaurant has 10 stars rating.                      ✅\n",
      "Tagine Time restaurant has 10 stars rating.                       ✅\n",
      "Carnivore Carnival restaurant has 9 stars rating.                 ✅\n",
      "Wok This Way restaurant has 4 stars rating.                       ✖\n",
      "Hummus a Tune restaurant has 5 stars rating.                      ✖\n",
      "Rumba Roti restaurant has 4 stars rating.                         ✖\n",
      "Lederhosen Lounge restaurant has 3 stars rating.                  ✖\n",
      "Mamma Mia's Pizzeria restaurant has 1 stars rating.               ✖\n",
      "Dim Sum Drummers restaurant has 1 stars rating.                   ✖\n",
      "New York Cheesecake Factory restaurant has 4 stars rating.        ✖\n",
      "Grillin' & Chillin' restaurant has 5 stars rating.                ✖\n",
      "Gyro Hero restaurant has 6 stars rating.                          ✖\n",
      "Persian Cat Café restaurant has 2 stars rating.                   ✖\n",
      "Gaucho Grillhouse restaurant has 2 stars rating.                  ✖\n",
      "Tuk Tuk Thai restaurant has 2 stars rating.                       ✖\n",
      "Pizza My Heart restaurant has 1 stars rating.                     ✖\n",
      "Pineapple Express restaurant has 1 stars rating.                  ✖\n",
      "Spice Spice Baby restaurant has 4 stars rating.                   ✖\n",
      "Poutine Palace restaurant has 3 stars rating.                     ✖\n",
      "Waffle Warriors restaurant has 3 stars rating.                    ✖\n",
      "Casablanca Café restaurant has 1 stars rating.                    ✖\n",
      "The Vodka Vault restaurant has 5 stars rating.                    ✖\n",
      "The Fish Fryer restaurant has 5 stars rating.                     ✖\n",
      "Baguette About It restaurant has 1 stars rating.                  ✖\n",
      "Taco 'Bout Tasty restaurant has 5 stars rating.                   ✖\n",
      "Curry Flurry restaurant has 3 stars rating.                       ✖\n",
      "Rib Ticklers BBQ restaurant has 4 stars rating.                   ✖\n",
      "Pho Real restaurant has 4 stars rating.                           ✖\n",
      "Tapas Time restaurant has 2 stars rating.                         ✖\n",
      "Tater Tot Tavern restaurant has 1 stars rating.                   ✖\n",
      "Ravioli Rumble restaurant has 3 stars rating.                     ✖\n",
      "Smorgasbord Smiles restaurant has 4 stars rating.                 ✖\n",
      "Jerk Chicken Junction restaurant has 4 stars rating.              ✖\n",
      "Ouzo Oomph restaurant has 6 stars rating.                         ✖\n",
      "Kilted Kuisine restaurant has 6 stars rating.                     ✖\n",
      "Pierogi Palace restaurant has 4 stars rating.                     ✖\n",
      "Fon-Do or Fon-Don't restaurant has 5 stars rating.                ✖\n",
      "Seoul Food restaurant has 3 stars rating.                         ✖\n",
      "Falafel Funnies restaurant has 5 stars rating.                    ✖\n",
      "Flapjack Fiesta restaurant has 5 stars rating.                    ✖\n",
      "Lucky Leprechaun Pub restaurant has 2 stars rating.               ✖\n",
      "Fiesta Fiesta restaurant has 6 stars rating.                      ✖\n",
      "Ceviche Circus restaurant has 5 stars rating.                     ✖\n",
      "The Catchy Cod restaurant has 4 stars rating.                     ✖\n",
      "Noodle Nook restaurant has 5 stars rating.                        ✖\n",
      "Salsa and Samba restaurant has 3 stars rating.                    ✖\n",
      "The Buttered Bagel restaurant has 3 stars rating.                 ✖\n",
      "Danish Delight restaurant has 2 stars rating.                     ✖\n",
      "Sahara Sandwiches restaurant has 1 stars rating.                  ✖\n",
      "Down Under Diner restaurant has 4 stars rating.                   ✖\n",
      "Masala Magic restaurant has 4 stars rating.                       ✖\n",
      "Goulash Galore restaurant has 1 stars rating.                     ✖\n",
      "Codfather Fishery restaurant has 6 stars rating.                  ✖\n",
      "Jerk and Twirl restaurant has 1 stars rating.                     ✖\n",
      "The Fez Food Fest restaurant has 2 stars rating.                  ✖\n",
      "Northern Lights Nosh restaurant has 3 stars rating.               ✖\n",
      "Mirage Munchies restaurant has 4 stars rating.                    ✖\n",
      "Balkan Bites restaurant has 5 stars rating.                       ✖\n",
      "Count Dracula’s Diner restaurant has 1 stars rating.              ✖\n",
      "Mandalay Munchies restaurant has 1 stars rating.                  ✖\n",
      "Java Jive Café restaurant has 6 stars rating.                     ✖\n",
      "Adriatic Appetites restaurant has 6 stars rating.                 ✖\n",
      "The Tropic Taste restaurant has 5 stars rating.                   ✖\n",
      "Mango Mania restaurant has 4 stars rating.                        ✖\n",
      "Tofu Tango restaurant has 3 stars rating.                         ✖\n",
      "Safari Snacks restaurant has 5 stars rating.                      ✖\n",
      "Bamboo Bistro restaurant has 3 stars rating.                      ✖\n",
      "Luxe Lunches restaurant has 3 stars rating.                       ✖\n",
      "The Casbah Café restaurant has 1 stars rating.                    ✖\n",
      "Knights and Noodles restaurant has 3 stars rating.                ✖\n",
      "Caribana Café restaurant has 4 stars rating.                      ✖\n",
      "Olive Orchard Eatery restaurant has 5 stars rating.               ✖\n",
      "Biryani Bliss restaurant has 4 stars rating.                      ✖\n",
      "Kigali Kitchen restaurant has 1 stars rating.                     ✖\n",
      "Salsa Fiesta restaurant has 5 stars rating.                       ✖\n",
      "Savory Safari restaurant has 4 stars rating.                      ✖\n",
      "Nile Nibbles restaurant has 6 stars rating.                       ✖\n",
      "Serengeti Snacks restaurant has 3 stars rating.                   ✖\n",
      "Ugandan Yum Yum restaurant has 2 stars rating.                    ✖\n",
      "BBQ Bonanza restaurant has 2 stars rating.                        ✖\n",
      "Uzbek Utopias restaurant has 2 stars rating.                      ✖\n",
      "Caracas Crunch restaurant has 1 stars rating.                     ✖\n",
      "Yemeni Yum restaurant has 3 stars rating.                         ✖\n",
      "Zany Zambia restaurant has 4 stars rating.                        ✖\n",
      "Zesty Zimbabwe restaurant has 6 stars rating.                     ✖\n",
      "Damascus Dine-In restaurant has 4 stars rating.                   ✖\n",
      "Togo Tacos restaurant has 1 stars rating.                         ✖\n",
      "Trinidad Treats restaurant has 2 stars rating.                    ✖\n",
      "Udon Believe It restaurant has 2 stars rating.                    ✖\n",
      "Vietnamese Vibes restaurant has 1 stars rating.                   ✖\n",
      "Welsh Rare-Bits restaurant has 6 stars rating.                    ✖\n",
      "Burrito Bandito restaurant has 1 stars rating.                    ✖\n",
      "Yummy Yucatan restaurant has 6 stars rating.                      ✖\n"
     ]
    }
   ],
   "source": [
    "for c, h in zip(candidate_sentences, is_hit):\n",
    "    print(f\"{c:60}  {hit_to_emoji[h]:>5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Show me restaurants with at least 7 stars rating\n",
      "\n",
      "Rank  1: Adriatic Appetites restaurant has 6 stars rating.             0.72   ✖\n",
      "Rank  2: Oui, Chef! restaurant has 10 stars rating.                    0.72   ✅\n",
      "Rank  3: Wok This Way restaurant has 4 stars rating.                   0.72   ✖\n",
      "Rank  4: Grillin' & Chillin' restaurant has 5 stars rating.            0.72   ✖\n",
      "Rank  5: Curry Up Now restaurant has 8 stars rating.                   0.72   ✅\n",
      "Rank  6: The Souvlaki Shack restaurant has 10 stars rating.            0.71   ✅\n",
      "Rank  7: Holy Cannoli! restaurant has 8 stars rating.                  0.71   ✅\n",
      "Rank  8: The Fish Fryer restaurant has 5 stars rating.                 0.71   ✖\n",
      "Rank  9: Carnivore Carnival restaurant has 9 stars rating.             0.71   ✅\n",
      "Rank 10: Fon-Do or Fon-Don't restaurant has 5 stars rating.            0.71   ✖\n",
      "\n",
      "P@10: 0.50\n",
      "R@10: 0.50\n"
     ]
    }
   ],
   "source": [
    "debug = True\n",
    "\n",
    "# encode the text query and candidate\n",
    "query_embedding = model.encode(query_sentence).reshape(1, -1)\n",
    "candidate_embeddings = model.encode(candidate_sentences)\n",
    "\n",
    "# L2 normalize the embeddings\n",
    "faiss.normalize_L2(query_embedding)\n",
    "faiss.normalize_L2(candidate_embeddings)\n",
    "\n",
    "# compute the cosine similarity using FAISS\n",
    "d = model.get_sentence_embedding_dimension()\n",
    "\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(candidate_embeddings)\n",
    "\n",
    "k = 10\n",
    "D, I = index.search(query_embedding, k)\n",
    "\n",
    "\n",
    "# print the result\n",
    "# print query \n",
    "print(f\"Query: {query_sentence}\", end=\"\\n\\n\")\n",
    "\n",
    "if debug:\n",
    "    for i in range(k):\n",
    "        retrieved_sentence = candidate_sentences[I[0][i]]\n",
    "        hit_or_not = is_hit[I[0][i]]\n",
    "        print(f\"Rank {i+1:>2}: {retrieved_sentence:60} {D[0][i]:>5.2f} \", end=\"\")\n",
    "        print(f\"{hit_to_emoji[hit_or_not]:>3}\")\n",
    "\n",
    "# compute precision and recall at 10\n",
    "precision_at_10 = sum([is_hit[i] for i in list(I[0][:10])]) / 10\n",
    "recall_at_10 = sum([is_hit[i] for i in list(I[0][:10])]) / sum(is_hit)\n",
    "\n",
    "print()\n",
    "print(f\"P@10: {precision_at_10:.2f}\")\n",
    "print(f\"R@10: {recall_at_10:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
